{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as optim\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# icml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('/Users/apple/Downloads/compas-scores-two-years.csv')\n",
    "data=data[(data['race']=='African-American') | (data['race']=='Caucasian')]\n",
    "data['race'].loc[data['race']=='African-American']= 1\n",
    "data['race'].loc[data['race']=='Caucasian']= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'name', 'first', 'last', 'compas_screening_date', 'sex', 'dob',\n",
       "       'age', 'age_cat', 'race', 'juv_fel_count', 'decile_score',\n",
       "       'juv_misd_count', 'juv_other_count', 'priors_count',\n",
       "       'days_b_screening_arrest', 'c_jail_in', 'c_jail_out', 'c_case_number',\n",
       "       'c_offense_date', 'c_arrest_date', 'c_days_from_compas',\n",
       "       'c_charge_degree', 'c_charge_desc', 'is_recid', 'r_case_number',\n",
       "       'r_charge_degree', 'r_days_from_arrest', 'r_offense_date',\n",
       "       'r_charge_desc', 'r_jail_in', 'r_jail_out', 'violent_recid',\n",
       "       'is_violent_recid', 'vr_case_number', 'vr_charge_degree',\n",
       "       'vr_offense_date', 'vr_charge_desc', 'type_of_assessment',\n",
       "       'decile_score.1', 'score_text', 'screening_date',\n",
       "       'v_type_of_assessment', 'v_decile_score', 'v_score_text',\n",
       "       'v_screening_date', 'in_custody', 'out_custody', 'priors_count.1',\n",
       "       'start', 'end', 'event', 'two_year_recid'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(data,columns=['race','juv_fel_count', 'decile_score','juv_misd_count','two_year_recid'])\n",
    "data=data.iloc[:20]\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_train,data_val=train_test_split(data,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race</th>\n",
       "      <th>juv_fel_count</th>\n",
       "      <th>decile_score</th>\n",
       "      <th>juv_misd_count</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   race  juv_fel_count  decile_score  juv_misd_count  two_year_recid\n",
       "1     1              0             3               0               1\n",
       "2     1              0             4               0               1\n",
       "3     1              0             8               1               0\n",
       "6     0              0             6               0               1\n",
       "8     0              0             1               0               0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance - d(x_n, v_k, alpha)\n",
    "def distance(X, v, alpha):\n",
    "    # X is the entire training set (? or any dataset)\n",
    "    # *****should be a dataframe*****\n",
    "    # - has N samples (N rows)\n",
    "    # - each samples has a length of D (number of features?)\n",
    "    # v is a list of vectors, where each vector v_k has a length of D\n",
    "    # alpha is a list of weights, each alpha_i is the weight for some feature\n",
    "    N = X.shape[0]\n",
    "    D = X.shape[1]\n",
    "    \n",
    "    # note how to store v\n",
    "    K = len(v)\n",
    "\n",
    "    # initialize a list of distance \n",
    "    # this should have a length N\n",
    "    res = np.zeros((N, K))\n",
    "    \n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            for d in range(D):         \n",
    "                res[n, k] += alpha[d]*(X.iloc[n][d] - v[k, d])**2\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function calculates M_nk\n",
    "# M_nk = P(Z=k|x_n), which is the probablity that x_n maps to v_k\n",
    "# x_n is a vector of length D\n",
    "# v is list of vectors, where each vector v_k has a length of D\n",
    "def M_nk(dist, k):\n",
    "    N = dist.shape[0]\n",
    "    K = dist.shape[1]\n",
    "    M_nk = np.zeros((N, K))\n",
    "    expo_res = np.zeros((N, K))\n",
    "    \n",
    "    for n in range(N):\n",
    "        deno = 0\n",
    "        for k in range(K):\n",
    "            expo_res[n, k] = math.exp((-1)*dist[n, k])\n",
    "            deno += expo_res[n, k]\n",
    "        for k in range(K):\n",
    "            M_nk[n, k] = expo_res[n, k] / deno\n",
    "            # raise exception\n",
    "    return M_nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function calculates M_k\n",
    "# X is the entire training set (? or any dataset)\n",
    "# *****should be a dataframe*****\n",
    "# - has N samples (N rows)\n",
    "# - each samples has a length of D (number of features?)\n",
    "# v is a list of vectors, where each vector v_k has a length of D\n",
    "# alpha is a list of weights, each alpha_i is the weight for some feature, length of D\n",
    "def M_k(X, M_nk, k):\n",
    "    N = X.shape[0]\n",
    "    K = M_nk.shape[1]\n",
    "    M_k = np.zeros(K)\n",
    "    \n",
    "    for k in range(K):\n",
    "        for n in range(N):\n",
    "            M_k[k] += M_nk[n, k]\n",
    "        M_k[k] = M_k[k]/N\n",
    "        \n",
    "    return M_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function calculates the reconstruction of x_n\n",
    "# ???????????? check alpha_i #######\n",
    "def x_n_hat(X, M_nk, v):\n",
    "    N = M_nk.shape[0]\n",
    "    D = X.shape[1]\n",
    "    K = M_nk.shape[1]\n",
    "    x_n_hat = np.zeros((N, D))\n",
    "    L_x = 0\n",
    "    \n",
    "    for n in range(N):\n",
    "        for d in range(D):\n",
    "            for k in range(K):\n",
    "                # note the format of v\n",
    "                x_n_hat[n, d] += M_nk[n, k]*v[k, d]\n",
    "        L_x += (X.iloc[n][d] - x_n_hat[n, d])**2\n",
    "        \n",
    "    # Check the length of the L_x and x_n_hat\n",
    "    \n",
    "    return x_n_hat, L_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function calculates the prediction for y_n\n",
    "# w is a list of weights of length K\n",
    "def y_n_hat(M_nk, w, y):\n",
    "    N = M_nk.shape[0]\n",
    "    K = M_nk.shape[1]\n",
    "    y_n_hat = np.zeros(N)\n",
    "    L_y = 0\n",
    "    \n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            y_n_hat[n] += M_nk[n, k]*w[k]\n",
    "        \n",
    "        # what if y_n_hat < 0\n",
    "        \n",
    "        # print((-1)*y[n]*np.log(y_n_hat[n]))\n",
    "        L_y += (-1)*y.iloc[n]*np.log(y_n_hat[n]) - (1 - y.iloc[n])*np.log(1 - y_n_hat[n])\n",
    "        \n",
    "    return y_n_hat, L_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(param, sen_df, nsen_df, sen_y, nonsen_y, K, A_z, A_x, A_y):\n",
    "    \n",
    "    sen_N, sen_D = sen_df.shape\n",
    "    nsen_N, nsen_D = nsen_df.shape\n",
    "    \n",
    "#     alpha_sen_1=np.random.random_sample((sen_D,))\n",
    "#     alpha_nsen_1=np.random.random_sample((sen_D,))\n",
    "#     alpha_sen=alpha_sen_1/sum(alpha_sen_1)\n",
    "#     alpha_nsen=alpha_nsen_1/sum(alpha_nsen_1)\n",
    "#     w_1=np.random.random_sample((K,))\n",
    "#     w=w_1/sum(w_1)\n",
    "    \n",
    "#     v=np.random.random((K, sen_D))\n",
    "\n",
    "    alpha_sen = param[:sen_D]\n",
    "    alpha_nsen = param[sen_D : 2 * sen_D]\n",
    "    w = param[2 * sen_D : (2 * sen_D) + K]\n",
    "    v = np.matrix(param[(2 * sen_D) + K:]).reshape((K, sen_D))\n",
    "    \n",
    "    \n",
    "    para_values = []\n",
    "    f_value = []\n",
    "\n",
    "    # for i in range(1, 2):\n",
    "    dist_sen = distance(sen_df, v, alpha_sen)\n",
    "    dist_nsen = distance(nsen_df, v, alpha_nsen)        \n",
    "\n",
    "    M_nk_sen = M_nk(dist_sen, K)\n",
    "    M_nk_nsen = M_nk(dist_nsen, K)\n",
    "\n",
    "    M_k_sen = M_k(sen_df, M_nk_sen, K)\n",
    "    M_k_nsen = M_k(nsen_df, M_nk_nsen, K)\n",
    "\n",
    "    L_z = 0\n",
    "    for k in range(K):\n",
    "        L_z += abs(M_k_sen[k] - M_k_nsen[k])\n",
    "\n",
    "    x_n_hat_sen, L_x_sen = x_n_hat(sen_df, M_nk_sen, v)\n",
    "    x_n_hat_nsen, L_x_nsen = x_n_hat(nsen_df, M_nk_nsen, v)\n",
    "\n",
    "    L_x = L_x_sen + L_x_nsen\n",
    "\n",
    "       # print(sen_y.iloc[0])\n",
    "    y_hat_sen, L_y_sen = y_n_hat(M_nk_sen, w, sen_y)\n",
    "        \n",
    "    y_hat_nsen, L_y_nsen = y_n_hat(M_nk_nsen, w, nonsen_y)\n",
    "\n",
    "    L_y = L_y_sen + L_y_nsen\n",
    "\n",
    "    metric = A_z*L_z + A_x*L_x + A_y*L_y\n",
    "\n",
    "    return metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predic_threshold(preds):\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] >= 0.5:\n",
    "            preds[i] = 1\n",
    "        else:\n",
    "            preds[i] = 0\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_pred(params, D, K, sen_dt, nsen_dt, sen_label, nsen_label):\n",
    "    \n",
    "    best_alpha_sen = params[:D]\n",
    "    best_alpha_nsen = params[D : 2 * D]\n",
    "    best_w = params[2 * D : (2 * D) + K]\n",
    "    best_v = np.matrix(params[(2 * D) + K:]).reshape((K, D))\n",
    "    \n",
    "    best_dist_sen = distance(sen_dt, best_v, best_alpha_sen)\n",
    "    best_dist_nsen = distance(nsen_dt, best_v, best_alpha_nsen) \n",
    "    \n",
    "    best_M_nk_sen = M_nk(best_dist_sen, K)\n",
    "    best_M_nk_nsen = M_nk(best_dist_nsen, K)\n",
    "    \n",
    "    y_hat_sen, L_y_sen = y_n_hat(best_M_nk_sen, best_w, sen_label)\n",
    "    y_hat_nsen, L_y_nsen = y_n_hat(best_M_nk_nsen, best_w, nsen_label)\n",
    "    \n",
    "    return y_hat_sen, y_hat_nsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_calibr(y_pred_sen, y_pred_nsen, y_sen_label, y_nsen_label):\n",
    "    converted_y_hat_sen = predic_threshold(y_pred_sen)\n",
    "    converted_y_hat_nsen = predic_threshold(y_pred_nsen)\n",
    "\n",
    "    y_pred_sen = pd.DataFrame(converted_y_hat_sen)\n",
    "    y_pred_nsen = pd.DataFrame(converted_y_hat_nsen)\n",
    "\n",
    "#     print(y_pred_sen)\n",
    "#     print(y_pred_nsen)\n",
    "#     print(y_sen_label)\n",
    "#     print(y_nsen_label)\n",
    "    \n",
    "    acc_sen = accuracy_score(y_sen_label, y_pred_sen)\n",
    "    acc_nsen = accuracy_score(y_nsen_label, y_pred_nsen)\n",
    "    \n",
    "    all_labels = y_sen_label.append(y_nsen_label)\n",
    "    all_preds = y_pred_sen.append(y_pred_nsen)\n",
    "    \n",
    "    total_accuracy = accuracy_score(all_preds, all_labels)\n",
    "    \n",
    "    print(\"The accuracy for the entire dataset is: \", total_accuracy)\n",
    "    print(\"The accuracy for xxx group is: \", acc_sen)\n",
    "    print(\"The accuracy for xxx group is: \", acc_nsen)\n",
    "    print(\"The calibration is: \", acc_sen-acc_nsen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### ?????????????\n",
    "def LFR(training_data, val_data, y_name, sen_variable_name, K, A_z, A_x, A_y):\n",
    "    # divide the training set into sensitive & nonsensitive group\n",
    "    sen_training = training_data[training_data[sen_variable_name]==0]\n",
    "    nsen_training = training_data[training_data[sen_variable_name]==1]\n",
    "    \n",
    "    # divide the validation set into sensitive & nonsensitive group\n",
    "    sen_val = val_data[val_data[sen_variable_name]==0]\n",
    "    nsen_val = val_data[val_data[sen_variable_name]==1]\n",
    "    \n",
    "    # remove sensitive variable in the sensitive training and validation group\n",
    "    sen_training=sen_training.drop(columns=[sen_variable_name])\n",
    "    sen_val=sen_val.drop(columns=[sen_variable_name])\n",
    "\n",
    "    # remove sensitive variable in the nonsensitive training and validation group    \n",
    "    nsen_training = nsen_training.drop(columns=[sen_variable_name])\n",
    "    nsen_val = nsen_val.drop(columns=[sen_variable_name])\n",
    "    \n",
    "    # assign y labels for sensitive training group\n",
    "    y_sen_training = sen_training[y_name]\n",
    "    sen_training = sen_training.drop(columns=[y_name])\n",
    "    \n",
    "    # assign y labels for sensitive validation group\n",
    "    y_sen_val = sen_val[y_name]\n",
    "    sen_val = sen_val.drop(columns=[y_name])\n",
    "    \n",
    "    # assign y labels for nonsensitive training group\n",
    "    y_nsen_training = nsen_training[y_name]\n",
    "    nsen_training = nsen_training.drop(columns=[y_name])\n",
    "    \n",
    "    # assign y labels for nonsensitive validation group\n",
    "    y_nsen_val = nsen_val[y_name]\n",
    "    nsen_val = nsen_val.drop(columns=[y_name])\n",
    "    \n",
    "    alpha_sen_1=np.random.random_sample((sen_training.shape[1],))\n",
    "    alpha_nsen_1=np.random.random_sample((nsen_training.shape[1],))\n",
    "    alpha_sen=alpha_sen_1/sum(alpha_sen_1)\n",
    "    alpha_nsen=alpha_nsen_1/sum(alpha_nsen_1)\n",
    "    w_1=np.random.random_sample((K,))\n",
    "    w=w_1/sum(w_1)\n",
    "    v=np.random.random((K, sen_training.shape[1]))\n",
    "    \n",
    "    initial = []\n",
    "    initial.extend(alpha_sen)\n",
    "    initial.extend(alpha_nsen)\n",
    "    initial.extend(w)\n",
    "    \n",
    "    for item in v:\n",
    "        initial.extend(item)\n",
    "    initial = np.array(initial)\n",
    "\n",
    "    bound=[]\n",
    "\n",
    "    for d in range(sen_training.shape[1]):\n",
    "        bound.append((0, 1))\n",
    "\n",
    "    for d in range(nsen_training.shape[1]):\n",
    "        bound.append((0, 1))\n",
    "\n",
    "    for k in range(K):\n",
    "        bound.append((0, 1))\n",
    "\n",
    "    for k in range(K):\n",
    "        for d in range(sen_training.shape[1]):\n",
    "            bound.append((None, None))\n",
    "    \n",
    "    \n",
    "    para, min_L, d = optim.fmin_l_bfgs_b(L, x0=initial, epsilon=1e-5, \n",
    "                                         args=(sen_training, nsen_training, y_sen_training, \n",
    "                                               y_nsen_training, K, A_z, A_x, A_y), \n",
    "                                         bounds = bound, approx_grad=True, \n",
    "                                         maxfun=150000, maxiter=150000)\n",
    "    \n",
    "    y_hat_sen_tr, y_hat_nsen_tr = cal_pred(para, sen_training.shape[1], K, sen_training, \n",
    "             nsen_training, y_sen_training, y_nsen_training)\n",
    "\n",
    "    print(\"For the training set:\")\n",
    "    cal_calibr(y_hat_sen_tr, y_hat_nsen_tr, y_sen_training, y_nsen_training)\n",
    "    \n",
    "    print(\"++++++++++++++++++++++++++++\")\n",
    "    print(\"For the validation set:\")\n",
    "    \n",
    "    y_hat_sen_val, y_hat_nsen_val = cal_pred(para, sen_val.shape[1], K, sen_val, \n",
    "             nsen_val, y_sen_val, y_nsen_val)  \n",
    "    print(\"For the validation set:\")\n",
    "    cal_calibr(y_hat_sen_val, y_hat_nsen_val, y_sen_val, y_nsen_val)    \n",
    "    \n",
    "    return  para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the training set:\n",
      "The accuracy for the entire dataset is:  0.7333333333333333\n",
      "The accuracy for xxx group is:  0.7142857142857143\n",
      "The accuracy for xxx group is:  0.75\n",
      "The calibration is:  -0.0357142857142857\n",
      "++++++++++++++++++++++++++++\n",
      "For the validation set:\n",
      "For the validation set:\n",
      "The accuracy for the entire dataset is:  0.4\n",
      "The accuracy for xxx group is:  0.25\n",
      "The accuracy for xxx group is:  1.0\n",
      "The calibration is:  -0.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.14706479,  0.        ,  0.67007669,  0.7980738 ,  0.15606314,\n",
       "        1.        ,  0.        ,  1.        ,  0.87961552,  0.47579889,\n",
       "        1.14292234,  0.21571518,  0.08433486, -0.46668309])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LFR(data_train, data_val, 'two_year_recid', 'race', 2, 0.2, 0.4, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
